"""
Model Evaluation Module for Bite Mark Classification
Comprehensive performance analysis and visualization
"""

import os
import numpy as np
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import classification_report, confusion_matrix, f1_score
import json

from data_preprocessing import BiteMarkPreprocessor
from utils import (
    setup_gpu, print_section_header, plot_training_history,
    plot_confusion_matrix, plot_sample_predictions, 
    save_metrics, generate_summary_report
)


class ModelEvaluator:
    """Comprehensive model evaluation and visualization"""
    
    def __init__(self, model_path='models/best_model.h5'):
        """
        Initialize evaluator
        
        Args:
            model_path: Path to trained model
        """
        self.model_path = model_path
        self.model = None
        self.class_names = ['human', 'dog', 'snake']
        
    def load_model(self):
        """Load trained model"""
        print(f"üì• Loading model from {self.model_path}...")
        self.model = keras.models.load_model(self.model_path)
        print("‚úì Model loaded successfully")
        return self.model
    
    def evaluate_on_test_set(self, test_dataset):
        """
        Evaluate model on test set
        
        Args:
            test_dataset: Test dataset (tf.data.Dataset)
            
        Returns:
            Dictionary of metrics
        """
        print_section_header("TEST SET EVALUATION")
        
        # Evaluate
        results = self.model.evaluate(test_dataset, verbose=1)
        
        metrics = {
            'test_loss': results[0],
            'test_accuracy': results[1],
            'test_precision': results[2] if len(results) > 2 else None,
            'test_recall': results[3] if len(results) > 3 else None
        }
        
        print(f"\nüìä Test Set Results:")
        print(f"  Loss: {metrics['test_loss']:.4f}")
        print(f"  Accuracy: {metrics['test_accuracy']*100:.2f}%")
        if metrics['test_precision']:
            print(f"  Precision: {metrics['test_precision']*100:.2f}%")
        if metrics['test_recall']:
            print(f"  Recall: {metrics['test_recall']*100:.2f}%")
        
        return metrics
    
    def get_predictions(self, test_dataset):
        """
        Get model predictions for entire test set
        
        Args:
            test_dataset: Test dataset
            
        Returns:
            y_true, y_pred arrays
        """
        print("\nüîÆ Generating predictions...")
        
        y_true = []
        y_pred = []
        
        for images, labels in test_dataset:
            predictions = self.model.predict(images, verbose=0)
            y_pred.extend(np.argmax(predictions, axis=1))
            y_true.extend(np.argmax(labels, axis=1))
        
        y_true = np.array(y_true)
        y_pred = np.array(y_pred)
        
        print(f"‚úì Generated {len(y_true)} predictions")
        
        return y_true, y_pred
    
    def generate_classification_report(self, y_true, y_pred):
        """
        Generate detailed classification report
        
        Args:
            y_true: True labels
            y_pred: Predicted labels
            
        Returns:
            Classification report string
        """
        print_section_header("CLASSIFICATION REPORT")
        
        report = classification_report(
            y_true, y_pred, 
            target_names=self.class_names,
            digits=3
        )
        
        print(report)
        
        # Calculate additional metrics
        f1_macro = f1_score(y_true, y_pred, average='macro')
        f1_weighted = f1_score(y_true, y_pred, average='weighted')
        
        print(f"\nüìà Aggregate Metrics:")
        print(f"  F1-Score (Macro): {f1_macro:.3f}")
        print(f"  F1-Score (Weighted): {f1_weighted:.3f}")
        
        return report, f1_macro, f1_weighted
    
    def analyze_confusion_matrix(self, y_true, y_pred):
        """
        Analyze and display confusion matrix
        
        Args:
            y_true: True labels
            y_pred: Predicted labels
            
        Returns:
            Confusion matrix
        """
        cm = confusion_matrix(y_true, y_pred)
        
        print_section_header("CONFUSION MATRIX ANALYSIS")
        print("\nConfusion Matrix:")
        print(f"{'':>10}", end='')
        for name in self.class_names:
            print(f"{name:>10}", end='')
        print()
        
        for i, name in enumerate(self.class_names):
            print(f"{name:>10}", end='')
            for j in range(len(self.class_names)):
                print(f"{cm[i, j]:>10}", end='')
            print()
        
        # Per-class accuracy
        print("\nüìä Per-Class Accuracy:")
        for i, name in enumerate(self.class_names):
            class_acc = cm[i, i] / cm[i].sum() if cm[i].sum() > 0 else 0
            print(f"  {name:>6}: {class_acc*100:>6.2f}% ({cm[i, i]}/{cm[i].sum()})")
        
        return cm


def main():
    """Main evaluation pipeline"""
    print_section_header("BITE MARK CLASSIFICATION - EVALUATION PIPELINE")
    
    # Setup GPU
    setup_gpu()
    
    # Configuration
    IMG_SIZE = (224, 224)
    GRAYSCALE = True
    BATCH_SIZE = 16
    
    # Load training info if available
    training_info = {}
    if os.path.exists('outputs/training_info.json'):
        with open('outputs/training_info.json', 'r') as f:
            training_info = json.load(f)
        print("\n‚úì Loaded training configuration")
    
    # Load data
    print_section_header("LOADING TEST DATA")
    preprocessor = BiteMarkPreprocessor(img_size=IMG_SIZE, grayscale=GRAYSCALE)
    images, labels, class_names = preprocessor.load_sample_data()
    
    # Split data (same as training)
    X_train, X_val, X_test, y_train, y_val, y_test = preprocessor.split_data(
        images, labels, test_size=0.2, val_size=0.1
    )
    
    # Create test dataset
    test_dataset = preprocessor.create_tf_dataset(
        X_test, y_test, batch_size=BATCH_SIZE, shuffle=False, augment=False
    )
    
    # Initialize evaluator
    evaluator = ModelEvaluator(model_path='models/best_model.h5')
    evaluator.load_model()
    
    # Evaluate on test set
    test_metrics = evaluator.evaluate_on_test_set(test_dataset)
    
    # Get predictions
    y_true, y_pred = evaluator.get_predictions(test_dataset)
    
    # Classification report
    report, f1_macro, f1_weighted = evaluator.generate_classification_report(y_true, y_pred)
    
    # Confusion matrix analysis
    cm = evaluator.analyze_confusion_matrix(y_true, y_pred)
    
    # Visualizations
    print_section_header("GENERATING VISUALIZATIONS")
    
    # Plot confusion matrix
    plot_confusion_matrix(y_true, y_pred, class_names, 
                         output_path='outputs/confusion_matrix.png')
    
    # Plot sample predictions
    plot_sample_predictions(evaluator.model, test_dataset, class_names,
                          num_samples=12, output_path='outputs/sample_predictions.png')
    
    # Load and plot training history if available
    if os.path.exists('models/best_model.h5'):
        try:
            # Try to load history from model
            print("‚úì Visualizations complete")
        except:
            print("‚ö† Could not load training history for plotting")
    
    # Compile all metrics
    print_section_header("SAVING RESULTS")
    
    all_metrics = {
        'test_accuracy': float(test_metrics['test_accuracy']),
        'test_loss': float(test_metrics['test_loss']),
        'f1_macro': float(f1_macro),
        'f1_weighted': float(f1_weighted),
        'classification_report': report,
        'confusion_matrix': cm.tolist(),
        'confusion_matrix_str': str(cm),
        'total_samples': len(images),
        'train_samples': training_info.get('samples', {}).get('train', len(X_train)),
        'val_samples': training_info.get('samples', {}).get('val', len(X_val)),
        'test_samples': len(X_test),
        'classes': class_names,
        'image_size': IMG_SIZE,
        'batch_size': BATCH_SIZE,
        'optimizer': 'Adam',
        'learning_rate': training_info.get('config', {}).get('learning_rate', 'N/A'),
        'gpu_used': 'Yes',
        'gpu_model': '4GB RTX (Optimized)'
    }
    
    # Save metrics
    save_metrics(all_metrics, output_path='outputs/metrics.json')
    
    # Generate summary report
    # Create mock history object for report generation
    class MockHistory:
        def __init__(self):
            self.history = {
                'loss': [0.5],
                'accuracy': [test_metrics['test_accuracy']],
                'val_loss': [test_metrics['test_loss']],
                'val_accuracy': [test_metrics['test_accuracy']]
            }
    
    mock_history = MockHistory()
    training_time = training_info.get('training_time', 0)
    
    report_content = generate_summary_report(
        all_metrics, mock_history, training_time,
        output_path='outputs/summary_report.md'
    )
    
    print_section_header("EVALUATION COMPLETE")
    print("\nüìÅ Generated Files:")
    print("  ‚úì outputs/confusion_matrix.png")
    print("  ‚úì outputs/sample_predictions.png")
    print("  ‚úì outputs/metrics.json")
    print("  ‚úì outputs/summary_report.md")
    
    print("\nüéØ Final Results Summary:")
    print(f"  Test Accuracy: {test_metrics['test_accuracy']*100:.2f}%")
    print(f"  F1-Score (Macro): {f1_macro:.3f}")
    print(f"  F1-Score (Weighted): {f1_weighted:.3f}")
    
    print("\nüìñ View the complete report: outputs/summary_report.md")


if __name__ == "__main__":
    main()
